{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/s/bach/g/under/cutreap/Desktop/cs445/cell_images/'\n",
    "dataRoot = pathlib.Path(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imageDirs = []\n",
    "for item in dataRoot.iterdir():\n",
    "    imageDirs.append(item)\n",
    "\n",
    "dataRoot = []\n",
    "for path in imageDirs:\n",
    "    dataRoot.append(pathlib.Path(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeledImages = []\n",
    "label = 0\n",
    "classes = sorted(os.walk(dataPath).__next__()[1])\n",
    "# List each sub-directory (the classes)\n",
    "for c in classes:\n",
    "    c_dir = os.path.join(dataPath, c)\n",
    "    walk = os.walk(c_dir).__next__()\n",
    "    # Add each image to the training set\n",
    "    for sample in walk[2]:\n",
    "        # Only keeps jpeg images\n",
    "        if sample.endswith('.png') or sample.endswith('.jpeg'):\n",
    "            labeledImages.append( (label, os.path.join(c_dir, sample)) )\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeledTrain = []\n",
    "labeledTest = []\n",
    "i = 0\n",
    "for image in labeledImages:\n",
    "    if(i < .8*len(labeledImages)):\n",
    "        labeledTrain.append(image)\n",
    "        i += 1\n",
    "    else:\n",
    "        labeledTest.append(image)\n",
    "trainPaths = [x[1] for x in labeledTrain]\n",
    "trainLabels = [x[0] for x in labeledTrain]\n",
    "testPaths = [x[1] for x in labeledTest]\n",
    "testLabels = [x[0] for x in labeledTest]\n",
    "testSize = len(testLabels)\n",
    "trainSize = len(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#trainPaths = []\n",
    "#trainLabels = []\n",
    "#testPaths = []\n",
    "#testLabels = []\n",
    "#for i in range(len(imagePaths)):\n",
    "#    if(i < .8*len(imagePaths)):\n",
    "#        trainPaths.append(imagePaths[i])\n",
    "#        trainLabels.append(labels[i])\n",
    "#    else:\n",
    "#        testPaths.append(imagePaths[i])\n",
    "#        testLabels.append(labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def loadAndPreprocessImage(path):\n",
    "  image = tf.read_file(path)\n",
    "  image = tf.image.decode_png(image, channels=3)\n",
    "  image = tf.image.resize_images(image, [128, 128])\n",
    "  image = image / 127.5 - 1.0  # normalize to [-1,1] range\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepDataForConvNet(imagePaths, labels, batch_size):\n",
    "    # Convert to Tensor\n",
    "    imagePaths = tf.convert_to_tensor(imagePaths, dtype=tf.string)\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "    # Build a TF Queue, shuffle data\n",
    "    image, label = tf.train.slice_input_producer([imagePaths, labels],\n",
    "                                                 shuffle=True)\n",
    "\n",
    "    # Read images from disk\n",
    "    image = tf.read_file(image)\n",
    "    image = tf.image.decode_png(image, channels=CHANNELS)\n",
    "\n",
    "    # Resize images to a common size\n",
    "    image = tf.image.resize_images(image, [128, 128])\n",
    "    \n",
    "    \n",
    "\n",
    "    # Normalize\n",
    "    image = image * 1.0/127.5 - 1.0\n",
    "\n",
    "    # Create batches\n",
    "    X, Y = tf.train.batch([image, label], batch_size=batch_size,\n",
    "                          capacity=batch_size * 8,\n",
    "                          num_threads=4)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_steps = 50\n",
    "batchSize = 128\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# Image Parameters_\n",
    "N_CLASSES = 2 # CHANGE HERE, total number of classes\n",
    "IMG_HEIGHT = 128 # CHANGE HERE, the image height to be resized to\n",
    "IMG_WIDTH = 128 # CHANGE HERE, the image width to be resized to\n",
    "CHANNELS = 3 # The 3 color channels, change to 1 if grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-ece6a30fbb0a>:7: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/training/input.py:374: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/training/input.py:320: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-7-ece6a30fbb0a>:24: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n"
     ]
    }
   ],
   "source": [
    "trainImages, trainLabels = prepDataForConvNet(trainPaths, trainLabels, batchSize)\n",
    "testImages, testLabels = prepDataForConvNet(testPaths, testLabels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#trainPathDS = tf.data.Dataset.from_tensor_slices(trainPaths)\n",
    "#trainImageDS = trainPathDS.map(loadAndPreprocessImage, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#trainLabelDS = tf.data.Dataset.from_tensor_slices(tf.cast(trainLabels, tf.int64))\n",
    "#trainDS = tf.data.Dataset.zip((trainImageDS, trainLabelDS))\n",
    "\n",
    "#testPathDS = tf.data.Dataset.from_tensor_slices(testPaths)\n",
    "#testImageDS = testPathDS.map(loadAndPreprocessImage, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#testLabelDS = tf.data.Dataset.from_tensor_slices(tf.cast(testLabels, tf.int64))\n",
    "#testDS = tf.data.Dataset.zip((testImageDS, testLabelDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def conv_net(images, n_classes, dropout, reuse, is_training):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(images, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with kernel size of 2 and strides of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 5, 3)\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with kernel size of 2 and strides of 2 \n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 5, 3)\n",
    "        \n",
    "        conv3 = tf.layers.conv2d(conv2, 128, 3)\n",
    "        conv3 = tf.layers.max_pooling2d(conv3, 3, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv3)\n",
    "\n",
    "        # Fully connected layer (in contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "        # Because 'softmax_cross_entropy_with_logits' already apply softmax,\n",
    "        # we only apply softmax to testing network\n",
    "        out = tf.nn.softmax(out) if not is_training else out\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-298035ad32ec>:7: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-11-298035ad32ec>:9: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-11-298035ad32ec>:23: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-11-298035ad32ec>:25: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3-4.4.0/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-12-5fa328539137>:34: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Step 1, Minibatch Loss= 0.7731, Training Accuracy= 0.617\n",
      "Step 2, Minibatch Loss= 1.8477, Training Accuracy= 0.633\n",
      "Step 3, Minibatch Loss= 0.6931, Training Accuracy= 0.570\n",
      "Step 4, Minibatch Loss= 0.8874, Training Accuracy= 0.336\n",
      "Step 5, Minibatch Loss= 0.6381, Training Accuracy= 0.625\n",
      "Step 6, Minibatch Loss= 0.6751, Training Accuracy= 0.609\n",
      "Step 7, Minibatch Loss= 0.7056, Training Accuracy= 0.625\n",
      "Step 8, Minibatch Loss= 0.6438, Training Accuracy= 0.594\n",
      "Step 9, Minibatch Loss= 0.5502, Training Accuracy= 0.766\n",
      "Step 10, Minibatch Loss= 0.6162, Training Accuracy= 0.641\n",
      "Step 11, Minibatch Loss= 0.5797, Training Accuracy= 0.695\n",
      "Step 12, Minibatch Loss= 0.5760, Training Accuracy= 0.750\n",
      "Step 13, Minibatch Loss= 0.4861, Training Accuracy= 0.820\n",
      "Step 14, Minibatch Loss= 0.6287, Training Accuracy= 0.688\n",
      "Step 15, Minibatch Loss= 0.4905, Training Accuracy= 0.719\n",
      "Step 16, Minibatch Loss= 0.6096, Training Accuracy= 0.711\n",
      "Step 17, Minibatch Loss= 0.5453, Training Accuracy= 0.750\n",
      "Step 18, Minibatch Loss= 0.5054, Training Accuracy= 0.766\n",
      "Step 19, Minibatch Loss= 0.5070, Training Accuracy= 0.758\n",
      "Step 20, Minibatch Loss= 0.4432, Training Accuracy= 0.766\n",
      "Step 21, Minibatch Loss= 0.4793, Training Accuracy= 0.734\n",
      "Step 22, Minibatch Loss= 0.4204, Training Accuracy= 0.820\n",
      "Step 23, Minibatch Loss= 0.4070, Training Accuracy= 0.836\n",
      "Step 24, Minibatch Loss= 0.4464, Training Accuracy= 0.820\n",
      "Step 25, Minibatch Loss= 0.4613, Training Accuracy= 0.797\n",
      "Step 26, Minibatch Loss= 0.3588, Training Accuracy= 0.875\n",
      "Step 27, Minibatch Loss= 0.4046, Training Accuracy= 0.797\n",
      "Step 28, Minibatch Loss= 0.4843, Training Accuracy= 0.789\n",
      "Step 29, Minibatch Loss= 0.4206, Training Accuracy= 0.805\n",
      "Step 30, Minibatch Loss= 0.3559, Training Accuracy= 0.828\n",
      "Step 31, Minibatch Loss= 0.3125, Training Accuracy= 0.891\n",
      "Step 32, Minibatch Loss= 0.4013, Training Accuracy= 0.805\n",
      "Step 33, Minibatch Loss= 0.3527, Training Accuracy= 0.883\n",
      "Step 34, Minibatch Loss= 0.4250, Training Accuracy= 0.852\n",
      "Step 35, Minibatch Loss= 0.3470, Training Accuracy= 0.844\n",
      "Step 36, Minibatch Loss= 0.3251, Training Accuracy= 0.867\n",
      "Step 37, Minibatch Loss= 0.3462, Training Accuracy= 0.875\n",
      "Step 38, Minibatch Loss= 0.2793, Training Accuracy= 0.914\n",
      "Step 39, Minibatch Loss= 0.3200, Training Accuracy= 0.844\n",
      "Step 40, Minibatch Loss= 0.2186, Training Accuracy= 0.922\n",
      "Step 41, Minibatch Loss= 0.2962, Training Accuracy= 0.875\n",
      "Step 42, Minibatch Loss= 0.4056, Training Accuracy= 0.844\n",
      "Step 43, Minibatch Loss= 0.3460, Training Accuracy= 0.867\n",
      "Step 44, Minibatch Loss= 0.2279, Training Accuracy= 0.906\n",
      "Step 45, Minibatch Loss= 0.2981, Training Accuracy= 0.875\n",
      "Step 46, Minibatch Loss= 0.1915, Training Accuracy= 0.938\n",
      "Step 47, Minibatch Loss= 0.2082, Training Accuracy= 0.930\n",
      "Step 48, Minibatch Loss= 0.4006, Training Accuracy= 0.891\n",
      "Step 49, Minibatch Loss= 0.2878, Training Accuracy= 0.922\n",
      "Step 50, Minibatch Loss= 0.3825, Training Accuracy= 0.883\n",
      "Step 5, Training observing only Accuracy= 0.9296875\n",
      "Step 10, Training observing only Accuracy= 0.8984375\n",
      "Step 15, Training observing only Accuracy= 0.90625\n",
      "Step 20, Training observing only Accuracy= 0.8671875\n",
      "Step 25, Training observing only Accuracy= 0.890625\n",
      "Step 30, Training observing only Accuracy= 0.90625\n",
      "Step 35, Training observing only Accuracy= 0.8828125\n",
      "Step 40, Training observing only Accuracy= 0.8984375\n",
      "Step 45, Training observing only Accuracy= 0.875\n",
      "Step 50, Training observing only Accuracy= 0.828125\n",
      "Step 55, Training observing only Accuracy= 0.8984375\n",
      "Step 60, Training observing only Accuracy= 0.8515625\n",
      "Step 65, Training observing only Accuracy= 0.9140625\n",
      "Step 70, Training observing only Accuracy= 0.8515625\n",
      "Step 75, Training observing only Accuracy= 0.921875\n",
      "Step 80, Training observing only Accuracy= 0.8828125\n",
      "Step 85, Training observing only Accuracy= 0.890625\n",
      "Step 90, Training observing only Accuracy= 0.890625\n",
      "Step 95, Training observing only Accuracy= 0.8984375\n",
      "Step 100, Training observing only Accuracy= 0.8515625\n",
      "Step 105, Training observing only Accuracy= 0.8984375\n",
      "Step 110, Training observing only Accuracy= 0.8203125\n",
      "Step 115, Training observing only Accuracy= 0.859375\n",
      "Step 120, Training observing only Accuracy= 0.9140625\n",
      "Step 125, Training observing only Accuracy= 0.8984375\n",
      "Step 130, Training observing only Accuracy= 0.890625\n",
      "Step 135, Training observing only Accuracy= 0.921875\n",
      "Step 140, Training observing only Accuracy= 0.8359375\n",
      "Step 145, Training observing only Accuracy= 0.90625\n",
      "Step 150, Training observing only Accuracy= 0.8984375\n",
      "Step 155, Training observing only Accuracy= 0.90625\n",
      "Step 160, Training observing only Accuracy= 0.8984375\n",
      "Step 165, Training observing only Accuracy= 0.9140625\n",
      "Step 170, Training observing only Accuracy= 0.84375\n",
      "Step 100, Testing Accuracy= 1.0\n",
      "Step 200, Testing Accuracy= 1.0\n",
      "Step 300, Testing Accuracy= 1.0\n",
      "Step 400, Testing Accuracy= 1.0\n",
      "Step 500, Testing Accuracy= 1.0\n",
      "Step 600, Testing Accuracy= 1.0\n",
      "Step 700, Testing Accuracy= 1.0\n",
      "Step 800, Testing Accuracy= 1.0\n",
      "Step 900, Testing Accuracy= 1.0\n",
      "Step 1000, Testing Accuracy= 1.0\n",
      "Step 1100, Testing Accuracy= 1.0\n",
      "Step 1200, Testing Accuracy= 1.0\n",
      "Step 1300, Testing Accuracy= 1.0\n",
      "Step 1400, Testing Accuracy= 1.0\n",
      "Step 1500, Testing Accuracy= 1.0\n",
      "Step 1600, Testing Accuracy= 1.0\n",
      "Step 1700, Testing Accuracy= 1.0\n",
      "Step 1800, Testing Accuracy= 0.0\n",
      "Step 1900, Testing Accuracy= 1.0\n",
      "Step 2000, Testing Accuracy= 1.0\n",
      "Step 2100, Testing Accuracy= 1.0\n",
      "Step 2200, Testing Accuracy= 1.0\n",
      "Step 2300, Testing Accuracy= 1.0\n",
      "Step 2400, Testing Accuracy= 0.0\n",
      "Step 2500, Testing Accuracy= 1.0\n"
     ]
    }
   ],
   "source": [
    "logits_train = conv_net(trainImages, N_CLASSES, dropout, reuse=False, is_training=True)\n",
    "# Create another graph for testing that reuse the same weights\n",
    "logits_test = conv_net(trainImages, N_CLASSES, dropout, reuse=True, is_training=False)\n",
    "\n",
    "\n",
    "logits_test_test = conv_net(testImages, N_CLASSES, dropout, reuse=True, is_training=False)\n",
    "\n",
    "# Define loss and optimizer (with train logits, for dropout to take effect)\n",
    "loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    logits=logits_train, labels=trainLabels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits_test, 1), tf.cast(trainLabels, tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "correct_pred_test = tf.equal(tf.argmax(logits_test_test, 1), tf.cast(testLabels, tf.int64))\n",
    "accuracy_test = tf.reduce_mean(tf.cast(correct_pred_test, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Saver object\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the data queue\n",
    "    tf.train.start_queue_runners()\n",
    "\n",
    "    # Training cycle\n",
    "    \n",
    "    train_acc = 0;\n",
    "    \n",
    "    for step in range(1, num_steps+1):\n",
    "\n",
    "        if step % display_step == 0:\n",
    "            # Run optimization and calculate batch loss and accuracy\n",
    "            _, loss, acc = sess.run([train_op, loss_op, accuracy])\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "        else:\n",
    "            # Only run the optimization op (backprop)\n",
    "            sess.run(train_op)\n",
    "            \n",
    "            \n",
    "    for step in range(1, (math.ceil(trainSize/batchSize) )+1):\n",
    "        # Run optimization and calculate batch loss and accuracy\n",
    "        acc3 = sess.run(accuracy)\n",
    "        train_acc += acc3\n",
    "        if step % 5 == 0:\n",
    "            # Run optimization and calculate batch loss and accuracy\n",
    "            print(\"Step \" + str(step) + \", Training observing only Accuracy= \" + str(acc3))\n",
    "  \n",
    "      \n",
    "    train_acc = train_acc/(math.ceil(trainSize/batchSize) )\n",
    "    test_accuracy = 0;\n",
    "    for step in range(1, testSize + 1):\n",
    "        acc2 = sess.run(accuracy_test)\n",
    "        test_accuracy += acc2\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            # Run optimization and calculate batch loss and accuracy\n",
    "            print(\"Step \" + str(step) + \", Testing Accuracy= \" + str(acc2))\n",
    "\n",
    "            \n",
    "    test_accuracy = test_accuracy/testSize\n",
    "    print(\"Optimization Finished! \" + str(test_accuracy))\n",
    "    # Save your model\n",
    "    saver.save(sess, 'my_tf_model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LEFT OFF AT PIPE THE DATASET TO A MODEL\n",
    "#NEED TO:\n",
    "#    SEPERATE DATA INTO TRAINING AND TESTING ###DONE\n",
    "#    TRAIN MODEL\n",
    "#    TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
